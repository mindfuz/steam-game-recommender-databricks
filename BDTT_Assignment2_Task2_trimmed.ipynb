{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba27905-4b49-441c-adbb-663dcde61004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e44714-e2bf-42e9-a563-ff8aeb973a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this task, we will build a collaborative filtering recommender system using user data from Steam, a gaming platform. The dataset has four attributes: the user ID, game, user behaviour, and playtime (if the behaviour is play). This data allows us to infer user preferences in the absence of concrete, explicit ratings given by users towards games.\n",
    "\n",
    "\n",
    "The main goal is to develop a personalised recommender system which uses previous interaction history between user and games to determine which games a user might enjoy in future. The end result should show a list of N games linked to a specific user_id tailored to the user’s unique profile. As the Steam dataset is 200,000 rows, Spark is a good module to use as this can be considered Big Data.\n",
    "\n",
    "\n",
    "We will use Alternating Least Squares (ALS) algorithm for matrix factorisation, before analysing the performance of the algorithm using metrics such as RMSE and Precision@10. There will also be some examples of hyperparameter tuning, and all of the experiments will be tracked using MLflow. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3d37b1-e577-4a64-8972-1560193d22e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "import mlflow\n",
    "import logging\n",
    "import mlflow.spark\n",
    "\n",
    "from pyspark.sql.functions import col, countDistinct, count, when, avg, explode, collect_list, log1p\n",
    "\n",
    "from pyspark.sql.functions import sum as sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f1a50c-1716-4d73-983a-4d488738ec15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)\n",
    "mlflow.pyspark.ml.autolog()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909a1775-eb6b-47e5-b42f-abdf30ab4076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Overview\n",
    "\n",
    "The dataset used for this project is `steam-200k.csv`, which contains 200,000 records of user-game interactions collected from the Steam platform. The attributes are as follows:\n",
    "\n",
    "- **user_id**: A unique identifier for each user.\n",
    "- **game**: The title of the game involved in the interaction.\n",
    "- **behaviour**: The action performed by the user towards the game. This is either `purchase` or `play`.\n",
    "- **value**: For `purchase` interactions, this is set to 1, while for `play` interactions, this is set to the number of hours played.\n",
    "\n",
    "The first three attributes should make up a primary key for this table, but there might be anomalies for games with downloadable content or games which have been gifted to friends. These interactions are treated as implicit feedback as there are no ratings given by the users.\n",
    "\n",
    "Having an overview of the data ensures that we know what to look for in the exploratory data analysis section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d79724a-f965-42bc-a02d-24d44a8da2fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"dbfs:/FileStore/tables/steam_200k.csv\", inferSchema=True)\n",
    "df = df.withColumnRenamed(\"_c0\", \"user_id\") \\\n",
    "       .withColumnRenamed(\"_c1\", \"game\") \\\n",
    "       .withColumnRenamed(\"_c2\", \"behaviour\") \\\n",
    "       .withColumnRenamed(\"_c3\", \"value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05cd18b-9e0a-450c-b51f-c072b410c952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e293c283-405b-455a-9d8f-0cf9ac74d55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exploratory Data Analysis (EDA)\n",
    "\n",
    "Verifying the validity of the data is crucial before moving on to developing the recommender system. Firstly, we discovered some basic information about the data, specifically around the user_ids and games. \n",
    "\n",
    "The dataset contains:\n",
    "- **Number of unique users**: 12393\n",
    "- **Number of unique games**: 5155\n",
    "- **Distribution of behaviour**: ~65% `purchase` entries and ~35% `play` entries  \n",
    "\n",
    "This final point was expected, as a game has to be purchased before it is played on.\n",
    "\n",
    "We then split the dataset into two separate dataframes: one where the behaviour was `play` and one for `purchase`. This would allow us to see how users interact with games, and how these interactions might become more apparent during preprocessing and modelling.\n",
    "\n",
    "Using the ‘play’ dataframe, we checked for the top games by average playtime. The top two were Eastside Hockey Manager and Baldur’s Gate II, with many Football Manager titles taking up the spots from 3-12. A good recommender system might bundle these manager titles together for specific users.\n",
    "\n",
    "To validate data integrity of the dataset, we then searched for duplicate rows. As we touched upon earlier, if a row has the same user_id, game name, and behaviour, then it is a duplicate row and should be analysed further.\n",
    "\n",
    "- **719** duplicate rows across the entire df\n",
    "- **12** of these duplicate rows were where ‘behaviour’ = ‘play’\n",
    "- **707** duplicate purchase entries\n",
    "\n",
    "This could be explained by bundle purchases including downloadable content, or it might be a problem with the Steam software logging redundant entries. I made the decision to remove all of the duplicate purchase entries while keeping the multiple play entries, as these represent actual user engagement. This would remove 707 rows, and prepare the newly cleaned dataset for modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f78fdf-cf25-480b-b35a-1a474ba57ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Number of unique users\n",
    "num_users = df.select(\"user_id\").distinct().count()\n",
    "print(f\"Number of unique users: {num_users}\")\n",
    "\n",
    "# Number of unique games\n",
    "num_games = df.select(\"game\").distinct().count()\n",
    "print(f\"Number of unique games: {num_games}\")\n",
    "\n",
    "# Proportion of 'purchase' vs 'play'\n",
    "behavior_counts = df.groupBy(\"behaviour\").count()\n",
    "behavior_counts.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b0fe0e-9178-48c1-a2c1-8a2e35a7e69c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter only \"play\" and \"purchase\" behaviour\n",
    "play_df = df.filter(df.behaviour == \"play\")\n",
    "purchase_df = df.filter(df.behaviour == \"purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8772492-4a02-47a3-8816-4720c4662e9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by game and calculate average playtime\n",
    "avg_playtime = (\n",
    "    play_df.groupBy(\"game\")\n",
    "           .agg(avg(\"value\").alias(\"avg_playtime\"))\n",
    "           .orderBy(col(\"avg_playtime\").desc())\n",
    ")\n",
    "\n",
    "# Show top 20 most played (on average)\n",
    "avg_playtime.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b5ec4f-4fe1-46c5-bd7f-bcdd1846c434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by the three key columns and count occurrences\n",
    "df.groupBy(\"user_id\", \"game\", \"behaviour\") \\\n",
    "  .count() \\\n",
    "  .filter(\"count > 1\") \\\n",
    "  .limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "618e9556-e09f-47d1-8a30-4375dea8d382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group play_df by user_id and game, then count\n",
    "duplicate_plays = play_df.groupBy(\"user_id\", \"game\") \\\n",
    "                         .count() \\\n",
    "                         .filter(\"count > 1\")\n",
    "\n",
    "# Join to original play_df to see the actual duplicate rows\n",
    "duplicate_play_rows = play_df.join(duplicate_plays.drop(\"count\"), on=[\"user_id\", \"game\"], how=\"inner\")\n",
    "\n",
    "# Show results\n",
    "duplicate_play_rows.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd144fa0-7968-4aec-a47e-02ee0ed59ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group purchase_df by user_id and game, then count\n",
    "duplicate_purchases = purchase_df.groupBy(\"user_id\", \"game\") \\\n",
    "                         .count() \\\n",
    "                         .filter(\"count > 1\")\n",
    "\n",
    "# Join to original purchase_df to see the actual duplicate rows\n",
    "duplicate_purchase_rows = purchase_df.join(duplicate_purchases.drop(\"count\"), on=[\"user_id\", \"game\"], how=\"inner\")\n",
    "\n",
    "# Show results\n",
    "duplicate_purchase_rows.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f543e5f1-8798-41b3-9905-ff26825fcc25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicate purchases where user_id + game are the same\n",
    "clean_purchase_df = purchase_df.dropDuplicates([\"user_id\", \"game\"])\n",
    "\n",
    "# Combine clean purchases back with original plays\n",
    "df_cleaned = clean_purchase_df.union(play_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7224eb-13f3-4fb1-abfc-69fdee1b38fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned.describe().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2c055e-3735-4459-8b82-6024fcd0ba9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Preprocessing and Feature Engineering\n",
    "\n",
    "To prepare the dataset for training a filtering model, we need to construct a singular rating which will act as the combined value for each user-game interaction. We are relying on the implicit feedback mentioned before, which includes playtime and purchase habits. \n",
    "\n",
    "A new column called `rating` was created, and the value was calculated as follows:\n",
    "\n",
    "- If the user purchased the game, the rating is set to 1, indicating clear interest in the game.\n",
    "- If the user played the game, a logarithmic transformation `log1p` is applied to the playtime in hours. This transformation:\n",
    "  - compresses very large values, reducing the effect of outliers\n",
    "  - preserves the structure of engagement (more hours still gives a higher rating)\n",
    "\n",
    "After this change, the new dataframe will have adjusted ratings for each combination of user_id and game, which reflect the user’s interest in the game. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43225316-a708-4682-9bf6-ea13bab93e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings = df_cleaned.withColumn(\"rating\",\n",
    "    when(col(\"behaviour\")==\"purchase\", 1.0)\n",
    "   .otherwise(log1p(col(\"value\")))\n",
    ").select(\"user_id\", \"game\", \"rating\")\n",
    "ratings.cache()\n",
    "ratings.describe(\"rating\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad1f30f3-bbfe-4b2b-a434-9ac94732a2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Indexing the `game` Column\n",
    "\n",
    "Before training the ALS model, we must convert the important categorical features (such as game names) into numerical IDs, as ALS requires integer-based identifiers. That meant employing the `StringIndexer` on the `game` column, encoding each game into a newly created `game_id`. The `game_id` column was then cast to integer type to ensure that it was compatible with ALS.\n",
    "\n",
    "We then checked the first 5 rows of this new dataframe `indexed` which had the game names, the game IDs, and the ratings created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f91036f-2dd5-42f0-a6d3-2fbb37b05484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "game_indexer = StringIndexer(inputCol=\"game\", outputCol=\"game_id\", handleInvalid=\"skip\")\n",
    "indexed = game_indexer.fit(ratings).transform(ratings)\n",
    "indexed = indexed.withColumn(\"game_id\", col(\"game_id\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2598bc8-7fbf-4431-9a79-46e6151d03df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "indexed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e149cb55-4e5e-4b33-9dcf-29aff1464dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Aggregating Ratings\n",
    "\n",
    "The final step before training the recommender system was to combine the ratings for `purchase` and `play` for each combination of `user_id` and `game_id` into one unified value. This was done simply by using the `sum` function to add the two ratings (1 for purchase, a logarithmic rating for play) into one total score. Some users had multiple records for playing the same game multiple times (as we noted before), and so this justified the use of aggregation.\n",
    "\n",
    "The new dataframe, `aggRatings`, consisted of the `user_id`, `game_id`, and `rating`, where the user_id and game_id would only have one value for each combination. All of these were numerical values, and could now be passed into the ALS algorithm for collaborative filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39084a7-5033-4db0-8939-1932b9292470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aggRatings = (indexed\n",
    "  .groupBy(\"user_id\", \"game_id\")\n",
    "  .agg(sum_(\"rating\").alias(\"rating\"))\n",
    ")\n",
    "\n",
    "aggRatings.limit(10).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710ae275-7293-4180-880a-07669a9da42a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Training the ALS Model and Hyperparameter Tuning\n",
    "\n",
    "Here, we train the collaborative filtering model, with the aim of retrieving a list of games for each user tailored to their specific profile.\n",
    "\n",
    "Firstly, we split the dataset into a training set and a test set using a fixed random seed. The test set takes up 20% of the dataset.\n",
    "\n",
    "We then used the Alternating Least Squares algorithm from Spark MLib, which is well-suited for this type of task where implicit feedback is involved. Hyperparameter tuning is a major part of ensuring the ALS model performs optimally, so we conducted a grid search over several combinations of `alpha` and `regParam`:\n",
    "\n",
    "- `alpha`: [5.0, 10.0, 20.0]\n",
    "- `regParam`: [0.01, 0.1]\n",
    "- `rank` and `maxIter` were fixed (at 6 and 4 respectively) as cycling through these values caused the notebook to run very slowly.\n",
    "\n",
    "We used root mean squared error (RMSE) as an evaluation metric, and whichever `alpha` and `regParam` gave the lowest score would be stored for use as our values for generating final recommendations. These values turned out to be `alpha` = 20 and `regParam` = 0.01, with RMSE = 2.1161, so these were put into the final ALS algorithm with higher `rank` and `maxIter` to finalise the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73442741-2431-4454-895d-9d5e4cdb18ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainDF, testDF = aggRatings.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc2b8e2c-2962-47d2-b230-24c1a1635624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Experiment Tracking with MLflow\n",
    "\n",
    "To manage and track the experiments, MLflow was integrated into the training process. MLflow would automatically log the parameters and RMSE score for each ALS model run, enabling us to compare results efficiently. We could then identify the best model configuration. Recording these metrics is good practice in tasks like these, where reproducibility is key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "200cd1f7-5cc1-42b9-b0fc-3c345defa2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "alpha_values = [5.0, 10.0, 20.0]\n",
    "reg_values = [0.01, 0.1]\n",
    "\n",
    "best_rmse = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for reg in reg_values:\n",
    "        with mlflow.start_run():\n",
    "            als = ALS(\n",
    "                userCol=\"user_id\",\n",
    "                itemCol=\"game_id\",\n",
    "                ratingCol=\"rating\",\n",
    "                implicitPrefs=True,\n",
    "                coldStartStrategy=\"drop\",\n",
    "                rank=6,\n",
    "                maxIter=4,\n",
    "                alpha=alpha,\n",
    "                regParam=reg\n",
    "            )\n",
    "            model = als.fit(trainDF)\n",
    "\n",
    "            predictions = model.transform(testDF).dropna()\n",
    "            rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "            # MLflow logging\n",
    "            mlflow.log_param(\"alpha\", alpha)\n",
    "            mlflow.log_param(\"regParam\", reg)\n",
    "            mlflow.log_param(\"rank\", 6)\n",
    "            mlflow.log_param(\"maxIter\", 4)\n",
    "            mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "            mlflow.spark.log_model(model, \"als_model\")\n",
    "\n",
    "            print(f\"Alpha: {alpha}, RegParam: {reg} → RMSE: {rmse:.4f}\")\n",
    "\n",
    "            # Save best model parameters\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_alpha = alpha\n",
    "                best_reg = reg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f714b33-48b1-466f-868b-f662fbcaae24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"game_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    implicitPrefs=True,\n",
    "    coldStartStrategy=\"drop\",\n",
    "    rank=10,\n",
    "    regParam=reg,\n",
    "    alpha=alpha,\n",
    "    maxIter=10\n",
    ")\n",
    "\n",
    "model = als.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1639e8-0354-421f-9912-2ace494d5e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(testDF).dropna()\n",
    "predictions.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3e42a6-5fd4-4bed-a3ee-509ee68dcaa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Test RMSE = {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aafdfc0-6b8b-4180-bb09-b7fe82bbc9c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Generating Recommendations\n",
    "\n",
    "With the ALS model trained and evaluated, we used it to generate game recommendations for each user. The code returned 10 games for each user in the dataset based on predicted interest.\n",
    "\n",
    "The output was a nested list of game IDs and predicted ratings. To make the data interpretable in a tabular format, we applied the `explode` function, which created 10 rows per user, each with its own game and rating.\n",
    "\n",
    "We then selected and renamed the relevant fields to create a new DataFrame, `flat`, which contains:\n",
    "- `user_id`\n",
    "- `game_id`\n",
    "- `predicted_rating`\n",
    "\n",
    "This structure allows for easier filtering, joining with metadata, and interpretation of results in the following steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2212aff6-2a57-4e04-95fb-39f96c6e2f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "userRecs = model.recommendForAllUsers(10)\n",
    "\n",
    "# Explode the nested list of game_ids and ratings\n",
    "exploded = userRecs.select(\"user_id\", explode(\"recommendations\").alias(\"rec\"))\n",
    "\n",
    "# Flatten to get user_id, game_id, and rating\n",
    "flat = exploded.select(\n",
    "    col(\"user_id\"),\n",
    "    col(\"rec.game_id\").alias(\"game_id\"),\n",
    "    col(\"rec.rating\").alias(\"predicted_rating\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90f1b859-c4f6-4d34-8945-bbea4d1ceb5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Mapping Recommendations to Game Titles\n",
    "\n",
    "To make the generated recommendations readable, we joined the dataframe `flat` with the original game titles from the indexed dataset. This allows the `user_id` to match up with game titles rather than a vague integer `game_id`.\n",
    "\n",
    "The resulting dataframe, `prettyRecs`, contains `user_id`, `game` and `predicted_rating`, all of which are key features in the recommender system. This final structure enables easy interpretation of the data. The data was ordered by `user_id` and then `predicted_rating` in order to group games by `user_id`, and then rank the games from 1 to 10.\n",
    "\n",
    "Initial analysis of the results reveals that certain game franchises appear very frequently appear the top recommendations across users. This includes popular titles such as ‘Counter Strike’, ‘Half-Life’, and ‘Portal’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3652ea3b-7e5a-4857-bb44-1bdd6503261e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select what we need from indexed\n",
    "game_lookup = indexed.select(\"game_id\", \"game\").distinct()\n",
    "\n",
    "# Join to map game_id to game\n",
    "prettyRecs = flat.join(game_lookup, on=\"game_id\", how=\"left\") \\\n",
    "                 .select(\"user_id\", \"game\", \"predicted_rating\") \\\n",
    "                 .orderBy(\"user_id\", col(\"predicted_rating\").desc())\n",
    "\n",
    "prettyRecs.show(50, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2213e39d-f162-4ebd-adac-cbddeae9a845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Identifying Franchise-Based Recommendations: Football Manager Case Study\n",
    "\n",
    "To explore the types of games being recommended by the model, we filtered the `prettyRecs` dataframe to show titles which contain the phrase ‘Football Manager’. There were 6 editions of this game found in the list of top average playtimes per game, given in the EDA section. Usually, when people engage with one edition of this game, they are more inclined to play the others, which makes the franchise a strong candidate for implicit engagement-based recommendations.\n",
    "\n",
    "By displaying these entries, we observed that these titles were often clustered within a single user’s top 10 recommendations. For example, `user_id` = 26813952 had six Football Manager games in their top 10 recommended games.\n",
    "\n",
    "Another block of code was added for extra analysis and observation. Here, we found the users that had at least one Football Manager game in their top 10, and then aggregated these counts to determine the distribution of recommendations. Users with one version of this title usually had 4 or 6 in their recommended top 10, confirming the hypothesis from before.\n",
    "\n",
    "This analysis highlights how strongly the model associates certain users with one specific franchise, reflecting preference patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56ce959-5f0a-4575-b754-2e4e6690c0b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for games that contain the exact phrase \"Football Manager\"\n",
    "fm_recs = prettyRecs.filter(col(\"game\").contains(\"Football Manager\"))\n",
    "\n",
    "# Show the results\n",
    "fm_recs.limit(10).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e1ccf4e-1b40-4db2-9295-c2a45e20b098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count how many Football Manager games were recommended per user\n",
    "fm_count_by_user = fm_recs.groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"fm_count\")\n",
    "\n",
    "# Count how many users had N Football Manager games recommended\n",
    "distribution = fm_count_by_user.groupBy(\"fm_count\").count().orderBy(\"fm_count\")\n",
    "distribution.show(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "262de215-3cd1-4af2-a96d-7315da15d7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Further Model Evaluation\n",
    "\n",
    "Earlier in the notebook, we used RMSE as a metric to evaluate the model’s predictive accuracy. For recommender systems, it is also important to assess whether the correct items are being predicted near the top of the list. Therefore, three different widely-used ranking metrics were used:\n",
    "\n",
    "- Precision@10: This measures the proportion of recommended games that match the actual list.\n",
    "- Mean Average Precision (MAP): Considers both the precision and the order across all positions.\n",
    "- NDCG@10: Gives higher weight to games found earlier in the list.\n",
    "\n",
    "The results were given as follows:\n",
    "\n",
    "- Precision@10: 0.08504941599281232\n",
    "- MAP: 0.2632924678474174\n",
    "- NDCG@10: 0.3399809231797672\n",
    "\n",
    "This provides evidence of a serviceable recommender system, as implicit data can be very noisy and tricky to predict. For large, sparse datasets like this Steam dataset, this is an acceptable performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58f66772-2745-4b64-8623-5560465ed086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Top 10 predictions per user\n",
    "preds = (\n",
    "    userRecs\n",
    "    .select(\"user_id\", explode(\"recommendations\").alias(\"rec\"))\n",
    "    .select(\"user_id\", col(\"rec.game_id\").alias(\"game_id\"))\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(collect_list(\"game_id\").alias(\"preds\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5718a651-e02f-4274-8063-b67edbbb6341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Actual games interacted with per user\n",
    "labels = (\n",
    "    testDF\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(collect_list(\"game_id\").alias(\"labels\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1867e4dd-a984-4d4a-b42f-207192b1e37a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create RDD of predictions and labels\n",
    "metrics_rdd = (\n",
    "    preds\n",
    "    .join(labels, \"user_id\")\n",
    "    .rdd\n",
    "    .map(lambda row: (row.preds, row.labels))\n",
    ")\n",
    "\n",
    "# Evaluate ranking metrics from predictions\n",
    "metrics = RankingMetrics(metrics_rdd)\n",
    "print(\"Precision@10:\", metrics.precisionAt(10))\n",
    "print(\"MAP:\", metrics.meanAveragePrecision)\n",
    "print(\"NDCG@10:\", metrics.ndcgAt(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e8b2d9-a612-479b-a861-4db67c7124c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Conclusion\n",
    "\n",
    "In this project, we created a collaborative filtering recommender system using implicit feedback data from Steam. The system was built using ALS and trained on a real dataset containing 200,000 user-game interactions.\n",
    "\n",
    "Data preprocessing steps were employed, such as deduplicating various rows of data and normalising play hours. Player behaviour was analysed and converted into a singular ‘rating’ value, and categorical features were converted into numeric indices for matrix factorisation.\n",
    "\n",
    "We then applied hyperparameter tuning and trained the model, before evaluating its performance with different types of metrics such as RMSE and Precision@10. This verified that the model was recommending games that align with user interests.\n",
    "\n",
    "One case study was that of the Football Manager franchise. The model effectively grouped titles of this series within the same user IDs, reflecting patterns that often take place within gaming culture. The same games would appear at the top of most lists recommended to users, such as ‘Counter Strike’, so there is still room for improvement. The model would benefit from explicit data and more hyperparameter tuning with `rank` and `maxIter`. Nevertheless, the recommender system performed to an acceptable level and there were obvious signs of success, while being a scalable approach.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BDTT_Assignment2_Task2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
